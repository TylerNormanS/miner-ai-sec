import urllib3
import time
import regex as re
from bs4 import BeautifulSoup, ParserRejectedMarkup
from openpyxl import Workbook
from openpyxl import load_workbook
import datetime
import pandas as pd
from transformers import BertModel
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertForSequenceClassification, AdamW
from tqdm import tqdm

headers={} #https://www.sec.gov/os/accessing-edgar-data
SIC=re.compile(r'STANDARD INDUSTRIAL CLASSIFICATION:\s*(?:[^\[]*\[([\d]+)\s*\]|([\d]+))')
HTMLtag=re.compile(r'<html>')
Website=re.compile(r'edgar+[\S]*')
cik = re.compile(r'(\d{5,7})\|')
date=re.compile(r'\|+(\d{4})[-/](\d{2})[-/](\d{2})\|')
name=re.compile(r'\|([^|]+)\|')
city_pattern = (r'CITY:\s*(.*?)\s*STATE:')
state_pattern = (r'STATE:\s*(.*?)\s*ZIP:')
zip_pattern = (r'ZIP:\s*(\d{5})')
file = 'goodcriminalsnevergetcaught.xlsx'

#Initiate Spaghetti

try:
    wb = load_workbook(file)
    ws = wb.active
    print('Loaded existing workbook')
    row_index = ws.max_row
    print('Max row index:', row_index)
except FileNotFoundError:
    wb=Workbook()
    ws=wb.create_sheet(title=file)
    print('new workbook', ws)
    ws.append(['Date','Name','CIK','SIC Code','Sentence','URL','Headquarters'])
    row_index = 2
    print(row_index)
    
for i in range(2000,2025): #every possible index list
    year=str(i)
    for j in range(1,5): #every possible quarter
        Q='QTR'+str(j)
        url='https://www.sec.gov/Archives/edgar/full-index/'+year+'/'+Q+'/master.idx'
        ''.join(url)
        http=urllib3.PoolManager()
        try:
            miner=http.request(method='get',url=url,body=None,headers=headers) #pulling the data
            print(i, 'opened.')
            try:
                miner_text=miner.data.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    miner_text=miner.data.decode('latin-1')
                except UnicodeDecodeError:
                    print('failed to decode the data with any decoding')
        except urllib.error.HTTPError as e:
            print(f'HTTP error occurred: {e.code} - {e.reason}',url)
        except urllib.error.URLError as e:
            print(f'URL error occurred: {e.reason}',url)
        except Exception as e:
            print(f'An unexpected error occurred: {e}', url)
        row_index=row_index+1
        masterindex=re.split('\n',miner_text)
        tenKindex=[address for address in masterindex if '10-K' in address] #sorting index to only include 10K, 10Q, 8K.
        print('row:', row_index)
        print('heading to the sheet')
        for address in tenKindex:
            CIK=cik.findall(address)
            DATE= date.findall(address)
            NAME=name.findall(address)
            working=Website.findall(address)
            workinglist=[comma.strip("',") for comma in working]
            workingliste=[brace.strip("]") for brace in workinglist]
            prefix='https://www.sec.gov/Archives/'
            prefixandsuffix=[prefix+suffix for suffix in workingliste] #makes the urls

            for w in prefixandsuffix: 
                SECDATA=http.request(method='get',url=w,body=None,headers=headers)
                try:
                    SECDATA_Decoded=SECDATA.data.decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        SECDATA_Decoded=SECDATA.data.decode('latin-1')
                    except UnicodeDecodeError:
                        print('Failed to decode the data with any encoding :c')
                    except urllib3.exceptions.HTTPError as e:
                        print('HTTPError occurred:', e)
                except Exception as e:
                    print("An unknown error occurred:", e) 
                SECDATA_Decoded=re.split('\n',SECDATA_Decoded)
                SECDATA_Decoded='\t'.join(SECDATA_Decoded)
                SICc=SIC.findall(str(SECDATA_Decoded))
                gateway=False
                if SICc:
                    SICCODE=[int(item[0] or item[1]) for item in SICc if item[0].isdigit() or item[1].isdigit()]
                    if SICCODE:
                        if int(SICCODE[0]) in [2833, 2834, 2835, 2836, 3826, 3827, 3829, 3841, 3842, 3843, 3844, 3845, 3851, 5047, 5122, 5912, 6321, 6324, 8000, 8011, 8050, 8051, 8060, 8062, 8071, 8082, 8090, 8093]:
                            gateway=True
                            print(SICCODE[0]) #Checking to see if the filing is from a healthcare company
                            if gateway and len(HTMLtag.findall(SECDATA_Decoded))>=1: #checking for html tag & converting to text if necessary
                                try:
                                    parsers= ['html.parser', 'lxml', 'html5lib']
                                    for parser in parsers:
                                        try:
                                            SECDATA_Decoded = BeautifulSoup(SECDATA_Decoded, parser)
                                            text_content=SECDATA_Decoded.get_text()
                                            #print('Parsed with', parser)
                                        except ParserRejectedMarkup:
                                            print('failed to retrieve document')
                                        except Exception as e:
                                            print("an unknown error occurred", e)
                                except Exception as e:
                                    print("an unknown error occurred")

                            elif gateway and len(HTMLtag.findall(SECDATA_Decoded))==0:
                                try:
                                    SECDATA_Decoded = BeautifulSoup(SECDATA_Decoded, 'html.parser')
                                    text_content=SECDATA_Decoded.get_text()
                                    #print('sneaky html')
                                except Exception as e:
                                    print('Failed to parse as HTML', e)
                                    text_content=SECDATA_Decoded
                                    #print('text')
                            else:
                                print('dud')


                            city_ = re.findall(city_pattern, text_content) #searching the text converted document for an address
                            state_ = re.findall(state_pattern, text_content)
                            zip_ = re.findall(zip_pattern, text_content)
                            if city_ and state_ and zip_:
                                full_address=city_[0]+', '+state_[0]+', '+zip_[0]
                                print(full_address)
                            else:
                                full_address = "not found"
                            for state in state_:
                                if state.lower() in ['tx','texas']:
                                    sentences=re.split(r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s", text_content) #searching all of the sentences for keywords
                                    alliance_pattern = r"(strategic alliance|strategic partnership|partnership|joint venture|venture|alliance)(?!.*limited partnership interest|LP|acquisition|merger|limited partnership|venture capital|venture capitalist)"
                                    alliance_found=False
                                    for sentence in sentences:

                                        if re.search(alliance_pattern, sentence, flags=re.IGNORECASE):
                                            #print('alliance found:',DATE, NAME, CIK, SICCODE, sentence)
                                            words=re.findall(r'\b\w+\b', sentence)
                                            cleaned_ = [re.sub(r'<[^>]+>', '', word) for word in words]
                                            cleaned_words=' '.join(cleaned_)
                                            try:
                                                cleaned_words= cleaned_words.encode('utf-8')
                                                print('encoded')
                                            except Exception as e:
                                                    print('Error', e)
                                            alliance_found=True
                                            try:
                                                if DATE is not None and NAME is not None and CIK is not None and SICCODE is not None and w is not None and cleaned_words is not None:
                                                    date_tuple = DATE[0]
                                                    date_str = '-'.join(date_tuple)
                                                    CIK_stripped= [string.rstrip('|') for string in CIK]
                                                    CIK_int = [int(string) for string in CIK_stripped]
                                                    ws.cell(row=row_index, column=1, value=date_str)
                                                    ws.cell(row=row_index, column=2, value=NAME[0])
                                                    ws.cell(row=row_index, column=3, value=str(CIK_int).replace('[', '').replace(']',''))
                                                    ws.cell(row=row_index, column=4, value=str(SICCODE).replace('[', '').replace(']',''))
                                                    if cleaned_words == 'no alliance':
                                                        ws.cell(row=row_index, column=5, value=cleaned_words)
                                                    else:
                                                        ws.cell(row=row_index, column=5, value=cleaned_words)
                                                        ws.cell(row=row_index, column=6, value=w)
                                                        ws.cell(row=row_index, column=7, value=full_address)
                                                    row_index += 1
                                                    print('Catalogued.')
                                                    print(DATE[0])
                                            except ValueError:
                                                print('ValueError: could not convert data')
                                            except TypeError:
                                                print('TypeError: NoneType encountered in alliance_data.')
                                            except Exception as e:
                                                print(f'An unexpected error occurred: {e}')
                                            wb.save(file)
                                            print('DONE!')
                                        
                                        #if not alliance_found:
                                            #print('nothing yet.')
                                else: 
                                    print('Out of State')
                        else:
                            gateway=False
                            #print('false')
                            continue
                    else:
                        gateway=False
                        print("SICCODE FAILED")
                else:
                    gateway=False
                    print(workingliste)
                    print('SICc is empty')





ws.cell(row=1, column=sheet.max_column + 1, value="CODE")  
for row_num in range(2, max_row + 1):
    ws.cell(row=row_num, column=sheet.max_column, value="")
    
wb.save(file)

data = pd.read_excel(file)
data.fillna(0, inplace=True)
Index= [number for number in data.index]
training_data = []
     
for L in range(len(Index)):
        dictionary = {}
        dictionary['input_text'] = data.Sentence[L]
        dictionary['label'] = data.CODE[L]
        training_data.append(dictionary)

print('Excel step complete.')
       
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
input_texts = [str(example['input_text']) if isinstance(example['input_text'], str) else str(example['input_text']) for example in training_data]
#labels = torch.tensor([example['label'] for example in training_data], dtype=torch.long)
tokenized_data = []

for input_text in tqdm(input_texts, desc='Tokenizing'):
    tokenized_input=tokenizer(input_text, padding = True,truncation = True,return_tensors='pt')
    tokenized_data.append(tokenized_input)
    
print('excel inputs translated for BERT')

input_ids = [item['input_ids'] for item in tokenized_data]
attention_masks = [item['attention_mask'] for item in tokenized_data]

model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
batch_size = 6
model.eval()
predictions = []
max_seq_length = max(input_id.size(1) for input_id in input_ids)
with torch.no_grad():
    for i in tqdm(range(0, len(input_ids), batch_size), desc='Evaluating'):
        # Reset padded lists at the beginning of each iteration
        padded_input_ids = []
        padded_attention_masks = []
        # Extract batch input_ids and attention_masks
        batch_input_ids = input_ids[i:i+batch_size]
        batch_attention_masks = attention_masks[i:i+batch_size]
        for input_id, attention_mask in zip(batch_input_ids, batch_attention_masks):
            # Perform padding or truncation
            if input_id.size(1) < max_seq_length:
                padding_length = max_seq_length - input_id.size(1)
                padding_tensor = torch.zeros((input_id.size(0), padding_length), dtype=torch.long)
                input_id = torch.cat([input_id, padding_tensor], dim=1)
            else:
                input_id = input_id[:, :max_seq_length]
            if attention_mask.size(1) < max_seq_length:
                padding_length = max_seq_length - attention_mask.size(1)
                padding_tensor = torch.zeros((attention_mask.size(0), padding_length), dtype=torch.long)
                attention_mask = torch.cat([attention_mask, padding_tensor], dim=1)
            else:
                attention_mask = attention_mask[:, :max_seq_length]
            padded_input_ids.append(input_id)
            padded_attention_masks.append(attention_mask)
        input_ids_batch = torch.stack([tensor.squeeze() for tensor in padded_input_ids])
        attention_masks_batch = torch.stack([tensor.squeeze() for tensor in padded_attention_masks])
        outputs = model(input_ids=input_ids_batch, attention_mask=attention_masks_batch)
        batch_predictions = torch.argmax(outputs.logits, dim=1).tolist()
        predictions.extend(batch_predictions)



prediction_series = pd.Series(predictions)
data['CODE'] = prediction_series

data.to_excel('your_dataset_with_predictions_FINAL.xlsx', index=False)

predictions_series.to_excel('predict_raw.xlsx', index = False)

print('Prediction completed.')
